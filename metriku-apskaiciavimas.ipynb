{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "!pip install python-Levenshtein\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "kJQ_kVPcUdgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import Levenshtein\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from bert_score import score"
      ],
      "metadata": {
        "id": "grUR8hk5T3n2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excel_file_path = '/content/100-lt-nematyti-test.xlsx'\n",
        "df = pd.read_excel(excel_file_path)\n",
        "df.dropna(subset=['original', 'mT5'], inplace=True)"
      ],
      "metadata": {
        "id": "4eGgdawZT6ix"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU"
      ],
      "metadata": {
        "id": "_vRNKBRTWMxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_sentences = df['original'].tolist()\n",
        "simplified_sentences = df['mT5'].tolist()\n",
        "reference_sentences = [[sentence.split()] for sentence in original_sentences]\n",
        "hypothesis_sentences = [sentence.split() for sentence in simplified_sentences]\n",
        "bleu_score = corpus_bleu(reference_sentences, hypothesis_sentences)\n",
        "print(f'BLEU Score: {bleu_score:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OTZiwI9WNcg",
        "outputId": "420fa4b7-a5e1-41db-a059-94a2ac22ead3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.3933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE"
      ],
      "metadata": {
        "id": "VGCxYucJWP9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_corpus_rouge(df):\n",
        "    references = df['original'].tolist()\n",
        "    hypotheses = df['mT5'].tolist()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "    for reference, hypothesis in zip(references, hypotheses):\n",
        "        scores = scorer.score(reference, hypothesis)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
        "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
        "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
        "    print(f'Average ROUGE-1 F-score : {avg_rouge1:.4f}')\n",
        "    print(f'Average ROUGE-2 F-score : {avg_rouge2:.4f}')\n",
        "    print(f'Average ROUGE-L F-score : {avg_rougeL:.4f}')\n",
        "\n",
        "calculate_corpus_rouge(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz1taEFeWQgc",
        "outputId": "12b68919-2981-40cd-d3d4-003dc7967070"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1 F-score : 0.8333\n",
            "Average ROUGE-2 F-score : 0.7548\n",
            "Average ROUGE-L F-score : 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Žodžiai"
      ],
      "metadata": {
        "id": "t08_r675WV6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentences_and_words(df):\n",
        "    num_original_sentences = df['original'].shape[0]\n",
        "    num_generated_sentences = df['mT5'].shape[0]\n",
        "    original_words_count = df['original'].apply(lambda x: len(str(x).split())).sum()\n",
        "    generated_words_count = df['mT5'].apply(lambda x: len(str(x).split())).sum()\n",
        "    return num_original_sentences, num_generated_sentences, original_words_count, generated_words_count\n",
        "\n",
        "num_original_sentences, num_generated_sentences, original_words_count, generated_words_count = calculate_sentences_and_words(df)\n",
        "print(\"Originalių sakinių skaičius:\", num_original_sentences)\n",
        "print(\"Supaprastintų sakinių skaičius:\", num_generated_sentences)\n",
        "print(\"Originalių žodžių skaičius:\", original_words_count)\n",
        "print(\"Supaprastintų žodžių skaičius:\", generated_words_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LD4ZI7_WW0A",
        "outputId": "35096a61-85a9-41e6-8073-5d4e57f80542"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Originalių sakinių skaičius: 3\n",
            "Supaprastintų sakinių skaičius: 3\n",
            "Originalių žodžių skaičius: 64\n",
            "Supaprastintų žodžių skaičius: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTscore"
      ],
      "metadata": {
        "id": "7V7PEEgvWeM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = df['original'].tolist()\n",
        "hypotheses = df['mT5'].tolist()\n",
        "P, R, F1 = score(hypotheses, references, lang='lt', model_type='bert-large-uncased')\n",
        "average_P = P.mean().item()\n",
        "average_R = R.mean().item()\n",
        "average_F1 = F1.mean().item()\n",
        "print(f'BERTScore Precision: {average_P}')\n",
        "print(f'BERTScore Recall: {average_R}')\n",
        "print(f'BERTScore F1: {average_F1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEoGnQKEWfn1",
        "outputId": "f082a638-a74a-4727-e050-bd77d94d64a5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore Precision: 0.9083678722381592\n",
            "BERTScore Recall: 0.8198363184928894\n",
            "BERTScore F1: 0.8612228035926819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MeaningBERT"
      ],
      "metadata": {
        "id": "6xh2tOPGWji4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"davebulaval/MeaningBERT\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"davebulaval/MeaningBERT\")\n",
        "original_sentences = df['original'].tolist()\n",
        "translated_sentences = df['mT5'].tolist()\n",
        "scores = []\n",
        "for orig_sent, trans_sent in zip(original_sentences, translated_sentences):\n",
        "    inputs = tokenizer(orig_sent, trans_sent, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "    scores.append(logits.item())\n",
        "average_score = sum(scores) / len(scores)\n",
        "print(f\"Average MeaningBERT Score: {average_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEFkfkHCWkjU",
        "outputId": "3860c143-8dbf-4329-ff92-08ccb0b4aeaf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MeaningBERT Score: 83.6210428873698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROLD+proportions"
      ],
      "metadata": {
        "id": "FpvWLIY6WunP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_only_levenshtein(s1, s2):\n",
        "    len_s1 = len(str(s1))\n",
        "    len_s2 = len(str(s2))\n",
        "    dp = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n",
        "    for i in range(len_s1 + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(len_s2 + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, len_s1 + 1):\n",
        "        for j in range(1, len_s2 + 1):\n",
        "            if str(s1)[i - 1] == str(s2)[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "    return dp[len_s1][len_s2]\n",
        "\n",
        "df['ROLD'] = df.apply(lambda row: replace_only_levenshtein(row['original'], row['mT5']), axis=1)\n",
        "\n",
        "def calculate_word_operations(df):\n",
        "    df['Operations'] = df.apply(lambda row: Levenshtein.editops(str(row['original']), str(row['mT5'])), axis=1)\n",
        "    df['Original_Words'] = df['original'].apply(lambda x: len(x.split()))\n",
        "    df['Generated_Words'] = df['mT5'].apply(lambda x: len(x.split()))\n",
        "    df['Deleted'] = df['Operations'].apply(lambda ops: sum(1 for op in ops if op[0] == 'delete'))\n",
        "    df['Added'] = df['Operations'].apply(lambda ops: sum(1 for op in ops if op[0] == 'insert'))\n",
        "    df['Reordered'] = df['Operations'].apply(lambda ops: sum(1 for op in ops if op[0] == 'replace'))\n",
        "    df['Proportion_Deleted'] = df['Deleted'] / df['Original_Words']\n",
        "    df['Proportion_Added'] = df['Added'] / df['Generated_Words']\n",
        "    df['Proportion_Reordered'] = df['Reordered'] / df['Original_Words']\n",
        "    return df\n",
        "\n",
        "df = calculate_word_operations(df)\n",
        "\n",
        "output_file_path = '/content/processed_data.xlsx'\n",
        "\n",
        "df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Išsaugota {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxO8XpSUWwi1",
        "outputId": "d23692ae-668c-4b6a-d381-a81fecf019a1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Išsaugota /content/processed_data.xlsx\n"
          ]
        }
      ]
    }
  ]
}